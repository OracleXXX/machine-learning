{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from text_unidecode import unidecode\n",
    "## this is a muticlass classification problem\n",
    "verbose = 0 ## print updates or not - boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print 'all modules imported'\n",
    "\n",
    "##################################################################\n",
    "## STEP 1 - READING AND CLEANING DATASETS\n",
    "##################################################################\n",
    "train_df = pd.read_json('train.json')\n",
    "test_df = pd.read_json('test.json')\n",
    "\n",
    "## check the shape of training and test dataset\n",
    "if verbose:\n",
    "    print 'size of training dataset is', train_df.shape\n",
    "    print 'size of test dataset is', test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read the ingredient list and do some cleaning\n",
    "## remove digits and lower the characters. strip any whitespaces if present\n",
    "all_ingredients_train = []\n",
    "all_cuisines = []\n",
    "for i, row in train_df.iterrows():\n",
    "    all_ingredients_train.append(unidecode(re.sub('\\d+', '',' '.join(row['ingredients']).lower().strip())))\n",
    "    all_cuisines.append(row['cuisine'])\n",
    "\n",
    "all_ingredients_test= []\n",
    "for i, row in test_df.iterrows():\n",
    "    all_ingredients_test.append(unidecode(re.sub('\\d+', '',' '.join(row['ingredients']).lower().strip())))\n",
    "    \n",
    "## remove special characters from ingredients\n",
    "all_ingredients_train = [ ing.replace(\"-\", \" \").replace(\"&\", \" \").replace(\"'\", \" \").replace(\"''\", \" \").replace(\"%\", \" \")\\\n",
    "                    .replace(\"!\", \" \").replace(\"(\", \" \").replace(\")\", \" \").replace(\"/\", \" \").replace(\"/\", \" \")\\\n",
    "                    .replace(\",\", \" \").replace(\".\", \" \") for ing in all_ingredients_train]\n",
    "\n",
    "## remove extra whitespaces\n",
    "all_ingredients_train = [ re.sub('\\s+', ' ', ing).strip() for ing in all_ingredients_train]\n",
    "\n",
    "## number of unique ingredients and cuisine in the dataset\n",
    "if verbose:\n",
    "    print 'total number of ingedients are', len(set(all_ingredients))\n",
    "    print 'total number of cusines are', len(set(all_cuisines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39774, 3)\n",
      "39774\n",
      "(9944, 2)\n",
      "9944\n"
     ]
    }
   ],
   "source": [
    "print train_df.shape\n",
    "print len(all_ingredients_train)\n",
    "print test_df.shape\n",
    "print len(all_ingredients_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## initialize tfidf vectorizer and label encoder\n",
    "tfidf = TfidfVectorizer()\n",
    "lbl = LabelEncoder()\n",
    "\n",
    "## fit and transform on the test and train dataset\n",
    "train = tfidf.fit_transform(all_ingredients_train).astype('float32')\n",
    "y = lbl.fit_transform(all_cuisines)\n",
    "\n",
    "test = tfidf.transform(all_ingredients_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 500, 'max_depth': 7, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "nfolds = 5 ## use 5-fold cross validation to check the best parameters\n",
    "n_estimators_values = range(100,1000,100)\n",
    "max_features_values = ['sqrt', 'log2']\n",
    "max_depth_values = range(4, 10, 3)\n",
    "min_samples_split_values = range(3, 7, 2)\n",
    "min_samples_leaf_values = range(1, 3, 2)\n",
    "param_grid = {'n_estimators': n_estimators_values, 'max_features' : max_features_values,\\\n",
    "              'max_depth':max_depth_values, 'min_samples_split': min_samples_split_values,\\\n",
    "               'min_samples_leaf':min_samples_leaf_values}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=nfolds)\n",
    "grid_search.fit(train, y)\n",
    "print grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, max_features='sqrt', \\\n",
    "                             max_depth=7, min_samples_split=5, min_samples_leaf=1,\\\n",
    "                             verbose=True, random_state=1, oob_score = True, class_weight='balanced_subsample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   14.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced_subsample',\n",
       "            criterion='gini', max_depth=7, max_features='sqrt',\n",
       "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "            min_samples_leaf=1, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=True, random_state=1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## fit the model\n",
    "model.fit(train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "## predict using the model\n",
    "cuisine_pred = model.predict(test)\n",
    "cuisine_pred_labels = lbl.inverse_transform(cuisine_pred)\n",
    "## take the id from the test dataframe\n",
    "ids = test_df['id']\n",
    "\n",
    "## make a submission file\n",
    "output = pd.DataFrame({'id': ids, 'cuisine': cuisine_pred_labels}, columns=['id', 'cuisine'])\n",
    "output.to_csv('random_forest_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        0.        ,  0.00012988])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = pd.Series(data=model.feature_importances_,index=tfidf.get_feature_names())\n",
    "important_features.sort_values(ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "soy           0.036080\n",
       "sesame        0.034731\n",
       "fish          0.029654\n",
       "sauce         0.028493\n",
       "cumin         0.023670\n",
       "olive         0.023118\n",
       "tortillas     0.022256\n",
       "lime          0.021804\n",
       "feta          0.019691\n",
       "cilantro      0.018670\n",
       "thyme         0.017235\n",
       "mirin         0.017011\n",
       "garam         0.016601\n",
       "allspice      0.016369\n",
       "cheese        0.015951\n",
       "ginger        0.015873\n",
       "masala        0.015764\n",
       "coconut       0.012860\n",
       "curry         0.012193\n",
       "cachaca       0.011600\n",
       "rice          0.011333\n",
       "cajun         0.011228\n",
       "seasoning     0.009927\n",
       "coriander     0.009597\n",
       "cinnamon      0.009360\n",
       "gochujang     0.009321\n",
       "oregano       0.009148\n",
       "seeds         0.008962\n",
       "thai          0.008886\n",
       "sake          0.008819\n",
       "                ...   \n",
       "master        0.000000\n",
       "masur         0.000000\n",
       "matcha        0.000000\n",
       "matsutake     0.000000\n",
       "mature        0.000000\n",
       "matzos        0.000000\n",
       "maui          0.000000\n",
       "mayer         0.000000\n",
       "mayonnais     0.000000\n",
       "mccormick     0.000000\n",
       "mcintosh      0.000000\n",
       "meatballs     0.000000\n",
       "meatloaf      0.000000\n",
       "meats         0.000000\n",
       "medal         0.000000\n",
       "medallions    0.000000\n",
       "melba         0.000000\n",
       "melissa       0.000000\n",
       "mellow        0.000000\n",
       "membrillo     0.000000\n",
       "menta         0.000000\n",
       "mentaiko      0.000000\n",
       "menthe        0.000000\n",
       "mentsuyu      0.000000\n",
       "merguez       0.000000\n",
       "meringue      0.000000\n",
       "mesquite      0.000000\n",
       "mex           0.000000\n",
       "mexicana      0.000000\n",
       "abalone       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
